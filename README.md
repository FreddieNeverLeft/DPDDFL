# Differentially Private Data Distillation Federated Learning

## Literature Review

* [FedDM: Iterative Distribution Matching for Communication-Efficient Federated Learning](https://arxiv.org/pdf/2207.09653.pdf)
* [Federated Learning via Synthetic Data](https://arxiv.org/pdf/2008.04489.pdf)
* [Meta Knowledge Condensation for Federated Learning](https://arxiv.org/pdf/2209.14851.pdf)
* [Federated Learning via Decentralized Dataset Distillation in Resource-Constrained Edge Environments](https://arxiv.org/pdf/2208.11311.pdf)
* [FedSynth: Gradient Compression via Synthetic Data in Federated Learning](https://arxiv.org/pdf/2204.01273.pdf)
* [Distilled One-Shot Federated Learning](https://arxiv.org/pdf/2009.07999.pdf)
* [DYNAFED: Tackling Client Data Heterogeneity with Global Dynamics](https://arxiv.org/pdf/2211.10878.pdf)
* [Fed-GLOSS-DP: Federated, Global Learning using Synthetic Sets with Record Level Differential Privacy](https://arxiv.org/pdf/2302.01068.pdf)
